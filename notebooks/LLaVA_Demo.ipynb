{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA: Large Language and Vision Assistant\n",
    "\n",
    "This notebook demonstrates the capabilities of LLaVA, a multimodal model that combines vision and language understanding based on the paper \"Visual Instruction Tuning\" (NeurIPS 2023).\n",
    "\n",
    "LLaVA connects a vision encoder (CLIP) with a large language model (e.g., Vicuna) using a projection layer, enabling it to understand and reason about images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Add parent directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from models.llava import LLaVA\n",
    "from utils.visualization import display_image_with_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Now, let's initialize the LLaVA model. This will download the necessary model weights if they're not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "# Note: For lower memory usage, you can set load_in_8bit=True or load_in_4bit=True\n",
    "model = LLaVA(\n",
    "    vision_model_path=\"openai/clip-vit-large-patch14-336\",\n",
    "    language_model_path=\"lmsys/vicuna-7b-v1.5\",\n",
    "    device=device,\n",
    "    load_in_8bit=False  # Set to True if you have limited GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions to download images from URLs and display them with captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \"\"\"Download an image from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "def save_image(image, filename):\n",
    "    \"\"\"Save an image to a file.\"\"\"\n",
    "    image.save(filename)\n",
    "    return filename\n",
    "\n",
    "def process_image_url(url, prompt, max_new_tokens=256, temperature=0.7):\n",
    "    \"\"\"Process an image from a URL and generate a response.\"\"\"\n",
    "    # Download and save the image\n",
    "    image = download_image(url)\n",
    "    filename = \"temp_image.jpg\"\n",
    "    save_image(image, filename)\n",
    "    \n",
    "    # Generate response\n",
    "    response = model.generate_from_image(\n",
    "        image_path=filename,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Display the image and response\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Prompt: {prompt}\")\n",
    "    plt.figtext(0.5, 0.01, f\"Response: {response}\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Image Description\n",
    "\n",
    "Let's start with a simple example: asking the model to describe an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "prompt = \"What's in this image?\"\n",
    "\n",
    "response = process_image_url(image_url, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Detailed Description\n",
    "\n",
    "Now, let's ask for a more detailed description of the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Describe this image in detail, including all the objects and their relationships.\"\n",
    "\n",
    "response = process_image_url(image_url, prompt, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Visual Reasoning\n",
    "\n",
    "Let's test the model's visual reasoning capabilities with a more complex image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "prompt = \"What are the potential dangers or safety concerns in this location?\"\n",
    "\n",
    "response = process_image_url(image_url, prompt, max_new_tokens=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Object Counting\n",
    "\n",
    "Let's test the model's ability to count objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://llava-vl.github.io/static/images/coco.jpg\"\n",
    "prompt = \"How many people are in this image? Count carefully.\"\n",
    "\n",
    "response = process_image_url(image_url, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Multi-turn Conversation\n",
    "\n",
    "Let's demonstrate a multi-turn conversation about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://llava-vl.github.io/static/images/monalisa.jpg\"\n",
    "image = download_image(image_url)\n",
    "filename = \"temp_image.jpg\"\n",
    "save_image(image, filename)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Image for conversation\")\n",
    "plt.show()\n",
    "\n",
    "# Initialize conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can understand images and answer questions about them.\"}\n",
    "]\n",
    "\n",
    "# First turn\n",
    "prompt = \"What is this famous artwork?\"\n",
    "response = model.generate_from_image(\n",
    "    image_path=filename,\n",
    "    prompt=prompt,\n",
    "    conversation=conversation.copy()\n",
    ")\n",
    "print(f\"User: {prompt}\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "# Update conversation\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Second turn\n",
    "prompt = \"Who painted it and when?\"\n",
    "response = model.generate_from_image(\n",
    "    image_path=filename,\n",
    "    prompt=prompt,\n",
    "    conversation=conversation.copy()\n",
    ")\n",
    "print(f\"User: {prompt}\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "# Update conversation\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Third turn\n",
    "prompt = \"What makes this painting so famous and unique?\"\n",
    "response = model.generate_from_image(\n",
    "    image_path=filename,\n",
    "    prompt=prompt,\n",
    "    conversation=conversation.copy()\n",
    ")\n",
    "print(f\"User: {prompt}\")\n",
    "print(f\"Assistant: {response}\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the capabilities of LLaVA, a multimodal model that combines vision and language understanding. The model can:\n",
    "\n",
    "1. Describe images at different levels of detail\n",
    "2. Perform visual reasoning tasks\n",
    "3. Count objects in images\n",
    "4. Engage in multi-turn conversations about images\n",
    "\n",
    "These capabilities make LLaVA useful for a wide range of applications, from image captioning to visual question answering and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}